{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2400 entries, 0 to 2399\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Sentence  2400 non-null   object\n",
      " 1   Polarity  2400 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 37.6+ KB\n",
      "None\n",
      "                                            Sentence  Polarity\n",
      "0                           Wow... Loved this place.         1\n",
      "1                                 Crust is not good.         0\n",
      "2          Not tasty and the texture was just nasty.         0\n",
      "3  Stopped by during the late May bank holiday of...         1\n",
      "4  The selection on the menu was great and so wer...         1\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2400 entries, 0 to 2399\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Sentence  2400 non-null   object\n",
      " 1   Polarity  2400 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 37.6+ KB\n",
      "None\n",
      "                                            Sentence  Polarity\n",
      "0                           Wow... Loved this place.         1\n",
      "1                                 Crust is not good.         0\n",
      "2          Not tasty and the texture was just nasty.         0\n",
      "3  Stopped by during the late May bank holiday of...         1\n",
      "4  The selection on the menu was great and so wer...         1\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "[Tom, Lee]\n",
    "[20034493]\n",
    "[MMA]\n",
    "[2021W]\n",
    "[MMA 865]\n",
    "[Sunday, October 18, 2020]\n",
    "\n",
    "\n",
    "Submission to Question [2], Part [1]\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_train = pd.read_csv(r\"C:\\Users\\Tom-T\\Google Drive\\Queen's MMA\\Big Data Analytics - MMA 865\\Assignment 1\\sentiment_train.csv\")\n",
    "\n",
    "print(df_train.info())\n",
    "print(df_train.head())\n",
    "\n",
    "df_test = pd.read_csv(r\"C:\\Users\\Tom-T\\Google Drive\\Queen's MMA\\Big Data Analytics - MMA 865\\Assignment 1\\sentiment_test.csv\")\n",
    "\n",
    "print(df_train.info())\n",
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Sentence\n",
      "Polarity          \n",
      "0             1213\n",
      "1             1187\n",
      "          Sentence\n",
      "Polarity          \n",
      "0              287\n",
      "1              313\n"
     ]
    }
   ],
   "source": [
    "#EDA\n",
    "\n",
    "#Class Balance\n",
    "print(df_train.groupby('Polarity').count())\n",
    "print(df_test.groupby('Polarity').count())\n",
    "#Seems good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing\n",
    "\n",
    "df_train.head(50)\n",
    "\n",
    "#Noticed some #NAME excel error values, let's remove them\n",
    "\n",
    "df_train[df_train['Sentence']=='#NAME?'] #4 instances\n",
    "df_train = df_train.drop(df_train[df_train.Sentence == '#NAME?'].index) #Removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing Continued (Combine with above command later)\n",
    "\n",
    "#Preprocessing via function to allow use in pipeline\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "\n",
    "\n",
    "def my_preprocess(doc):\n",
    "    \n",
    "    # Lowercase - caps could be important indicators of strength of polarity, but not directionality. Therefore, I feel \n",
    "    # confident lowering all cases\n",
    "    doc = doc.lower()\n",
    "        \n",
    "    # stopwords removal\n",
    "    split_doc = doc.split()\n",
    "    stopwords_result = [t for t in split_doc if t not in stopwords.words('english')]\n",
    "    doc = ' '.join(stopwords_result)\n",
    "    \n",
    "    # lemmatization\n",
    "    doc = WordNetLemmatizer().lemmatize(doc)\n",
    "    \n",
    "    return doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test train splitting for cross validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_train['Sentence']\n",
    "y = df_train['Polarity']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "#Attempt 1\n",
    "vectorizer = CountVectorizer(preprocessor = my_preprocess)\n",
    "alg = MultinomialNB()\n",
    "\n",
    "#Attempt 2 - seems like this is the best model at test F1 score of .80\n",
    "vectorizer = CountVectorizer(preprocessor = my_preprocess, ngram_range=[1,2])\n",
    "alg = MultinomialNB() #also attempted to tune hyperparameters such as alpha (additive smoothing), but saw no improvement\n",
    "\n",
    "#Attempt 3\n",
    "vectorizer = CountVectorizer(preprocessor = my_preprocess, ngram_range=[1,2])\n",
    "alg = GradientBoostingClassifier()\n",
    "\n",
    "pipe = Pipeline([('cv', vectorizer), ('alg', alg)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('cv',\n",
       "                 CountVectorizer(ngram_range=[1, 2],\n",
       "                                 preprocessor=<function my_preprocess at 0x0000018FA52F9C80>)),\n",
       "                ('alg', GradientBoostingClassifier())])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit model\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[225  18]\n",
      " [ 85 152]]\n",
      "\n",
      "F1 Score = 0.78542\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.93      0.81       243\n",
      "           1       0.89      0.64      0.75       237\n",
      "\n",
      "    accuracy                           0.79       480\n",
      "   macro avg       0.81      0.78      0.78       480\n",
      "weighted avg       0.81      0.79      0.78       480\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluate model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "pred_val = pipe.predict(X_val)\n",
    "\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_val, pred_val))\n",
    "\n",
    "print(\"\\nF1 Score = {:.5f}\".format(f1_score(y_val, pred_val, average=\"micro\")))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, pred_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[270  17]\n",
      " [167 146]]\n",
      "\n",
      "F1 Score = 0.69333\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.94      0.75       287\n",
      "           1       0.90      0.47      0.61       313\n",
      "\n",
      "    accuracy                           0.69       600\n",
      "   macro avg       0.76      0.70      0.68       600\n",
      "weighted avg       0.76      0.69      0.68       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Prediction on Test set, using Attempt #2's model\n",
    "X = df_test['Sentence']\n",
    "y = df_test['Polarity']\n",
    "\n",
    "pred_val = pipe.predict(X)\n",
    "\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y, pred_val))\n",
    "\n",
    "print(\"\\nF1 Score = {:.5f}\".format(f1_score(y, pred_val, average=\"micro\")))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y, pred_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify incorrectly classified instances\n",
    "\n",
    "errors_df = df_test\n",
    "errors_df['Predictions'] = pred_val\n",
    "errors_df['Incorrect Flag'] = np.where(errors_df['Predictions'] != errors_df['Polarity'],1,0)\n",
    "\n",
    "errors_csv = errors_df.loc[errors_df['Incorrect Flag']==1].to_csv(r\"C:\\Users\\Tom-T\\Google Drive\\Queen's MMA\\Big Data Analytics - MMA 865\\Assignment 1\\errors.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
